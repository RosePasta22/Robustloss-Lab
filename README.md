# robustloss-lab

Robust classification loss toolkit (CCE/SCCE, Focal, GCE) with simple experiment runners for noisy labels and outliers.  
ë…¸ì´ì¦ˆ/ì•„ì›ƒë¼ì´ì–´ í™˜ê²½ì—ì„œ ê°•ê±´í•œ ë¶„ë¥˜ ì‹¤í—˜ì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì‹¤í—˜ ëŸ¬ë„ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.

---

## âœ¨ Features
- Losses: CE, GCE, Focal, **CCE**, **SCCE**
- Experiment runners: `run_experiment`, `run_clean_vs_noise`, `run_clean_vs_outlier`
- Noise / Outlier injection utilities and metadata logging
- Minimal, sklearn-like workflow with PyTorch backend

---

## ğŸ“¦ Install

> **Note**: PyTorchëŠ” CUDA/CPU í™˜ê²½ì— ë§ì¶° ë³„ë„ ì„¤ì¹˜í•˜ì„¸ìš”.  
> ì˜ˆ:  
> ```bash
> pip install torch --index-url https://download.pytorch.org/whl/cu121
> ```

```bash
pip install robustloss-lab
```

---

## ğŸš€ Quick start

```python
from robustloss import (
    DatasetSchema, TaskType,
    make_loss, run_experiment, plot_history,
    NoiseConfig, OutlierConfig, pct_drop
)

# 1) Define dataset schema
schema = DatasetSchema(
    name="uci_wine",
    target_name="class",
    task_type=TaskType.MULTICLASS
)

# 2) Choose a loss (e.g., SCCE)
loss_fn = make_loss("scce", eps=1e-3)

# 3) Run a quick experiment
model, hist, report = run_experiment(
    df, schema,
    loss_fn=loss_fn, loss_name="SCCE",
    epochs=50, batch_size=64, lr=1e-3, weight_decay=1e-4,
    optimizer_name="adam", seed=42
)

print(report)  # dict(test_acc=..., test_f1=..., noise_meta=..., outlier_meta=...)
plot_history([hist], ["SCCE"])
```

---

## ğŸ“‚ Modules

**Core (required submodules)**  
- `schemas.py` â€” ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜  
- `preprocess.py`, `datamod.py` â€” ì „ì²˜ë¦¬ / split  
- `loss_functions.py` â€” CE, GCE, Focal, CCE, SCCE  
- `models.py` â€” Logistic/Softmax linear classifiers  
- `train_many.py` â€” í•™ìŠµ ë£¨í”„, early stopping, ì‹œê°í™”  

**Optional submodules**  
- `noise_types.py`, `apply_noise.py` â€” ë¼ë²¨/í”¼ì²˜ ë…¸ì´ì¦ˆ  
- `outliers.py`, `apply_outliers.py` â€” ì•„ì›ƒë¼ì´ì–´ ìƒì„±/ì£¼ì…  

> ì„ íƒ ëª¨ë“ˆì€ í™˜ê²½ì— ë”°ë¼ ë¯¸í¬í•¨ì¼ ìˆ˜ ìˆìœ¼ë©°, íŒ¨í‚¤ì§€ import ì‹œ `None`ìœ¼ë¡œ ë°”ì¸ë”©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---


## ğŸ§© API Sketch (ìš”ì•½)

### Experiment Runners
```python
model, hist, report = run_experiment(df, schema, loss_fn, ...)

[h_c, h_n], labels, df_res = run_clean_vs_noise(df, schema, loss_fn=loss_fn, ...)

[h_c, h_o], labels, df_res = run_clean_vs_outlier(df, schema, loss_fn=loss_fn, ...)
```

### Metadata
- `noise_meta`: ë¼ë²¨/í”¼ì²˜ ë…¸ì´ì¦ˆ ì ìš© ì •ë³´ (ì „ì´í–‰ë ¬, ì¸ë±ìŠ¤, ì‹œë“œ ë“±)  
- `outlier_meta`: ì•„ì›ƒë¼ì´ì–´ ì£¼ì… ìš”ì•½ (ë¹„ìœ¨, |z|-í†µê³„, më²”ìœ„/ì‹œë“œ ë“±)

---

## ğŸ§ª Important Prototypes

### **Setting**

#### **DatasetSchema**
```python
@dataclass(frozen=True, slots=True)
class DatasetSchema(
    name: str
    target_name: str
    task_type: Optional[TaskType] = None       # Task_Type.BINARY / Task_Type.MULTICLASS  Noneì¼ ì‹œ ì „ì²˜ë¦¬ ì‹œ ìë™ê°ì§€ 
    numeric_features: Optional[Sequence[str]] = None
    categorical_features: Optional[Sequence[str]] = None
    drop_features: Sequence[str] = field(default_factory=tuple)
)
```

##### **ì˜ˆì‹œ**
```python
schema = DatasetSchema(
    name="uci_wine",
    target_name="class",
    task_type=TaskType.MULTICLASS
)
```

#### **NoiseConfig**
```python
@dataclass(slots=True)
class NoiseConfig:
    kind: Literal["none", "label", "feature", "both"] = "none"

    # --- Label Noise ---
    label_mode: Optional[LabelMode] = None     # ë…¸ì´ì¦ˆ ì¢…ë¥˜ ["symmetric", "pairflip", "classdep", "instancedep"]
    label_rate: float = 0.0                    # ë…¸ì´ì¦ˆìœ¨ Î·
    seed_label: Optional[int] = None           # ë¼ë²¨ ë…¸ì´ì¦ˆ ëœë¤ ì‹œë“œ
    pairflip_pairs: Optional[Dict[int, int]] = None  # pairflipìš© í´ë˜ìŠ¤ ìŒ
    classdep_etas: Optional[np.ndarray] = None       # class-dependent ë…¸ì´ì¦ˆìœ¨ ë²¡í„°
    instancedep_tau: float = 1.0                     # instance-dependent scaling factor

    # --- Feature Noise ---
    feature_mode: Optional[FeatureMode] = None # ë…¸ì´ì¦ˆ ì¢…ë¥˜ ["gaussian", "spike"]
    seed_feature: Optional[int] = None         # í”¼ì²˜ ë…¸ì´ì¦ˆ ëœë¤ ì‹œë“œ
    feature_frac: float = 0.0                  # ì „ì²´ ìƒ˜í”Œ ì¤‘ ë…¸ì´ì¦ˆ ì ìš© ë¹„ìœ¨
    feature_scale: float = 0.0                 # Gaussian scale (std ë¹„ìœ¨)
    spike_frac: float = 0.0                    # Spike ì ìš© ë¹„ìœ¨
    spike_value: float = 10.0                  # Spike ê°’ (outlier í¬ê¸°)
```

#### **OutlierConfig**
```python
@dataclass(frozen=True, slots=True)
class OutlierConfig:
    spike_value: float = 10.0                  #
    rate: float = 0.1                          # outlier ë¹„ìœ¨ (0.1=10%)
    zmin: float = 3.0                          # z-score í•˜í•œ (3Ïƒ)
    zmax: float = 5.0                          # z-score ìƒí•œ (5Ïƒ)
    mmin: int = 1                              # í•œ í–‰ì—ì„œ ë³€ì¡°í•  feature ìµœì†Œ ê°œìˆ˜
    mmax: Optional[int] = None                 # í•œ í–‰ì—ì„œ ë³€ì¡°í•  feature ìµœëŒ€ ê°œìˆ˜ (None=ì „ì²´)
    two_side: bool = True                      # True: Â±, False: +ë§Œ
    seed_outlier: Optional[int] = 42           # Outlier ì‹œë“œ
    target: Iterable[str] = ("train",)         # ì£¼ì…í•  split ("train","val","test")
```

#### **run_experiment**
```python
run_experiment(
    df,                                        # Dataframe
    schema_or_name: Union[str, DatasetSchema], # DatasetSchema ê°ì²´ ë˜ëŠ” registry.py ì˜ str
    loss_fn,                       # ì‚¬ìš©í•  ì†ì‹¤ í•¨ìˆ˜ (ì˜ˆ: CE, GCE, CCE ë“±)

    # -------------------------
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê¸°ë³¸ í”„ë¦¬ì…‹)
    # -------------------------
    epochs: int = 50,              # ìµœëŒ€ í•™ìŠµ epoch ìˆ˜
    batch_size: int = 64,          # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
    lr: float = 1e-3,              # í•™ìŠµë¥  (learning rate)
    weight_decay: float = 1e-4,    # L2 ì •ê·œí™” ê°•ë„ (weight decay)

    optimizer_name: str = "adam",  # ì˜µí‹°ë§ˆì´ì € ì¢…ë¥˜ ("adam" | "sgd" | "sgd_momentum")
    loss_name: str = "loss",       # ì†ì‹¤ í•¨ìˆ˜ ì´ë¦„ (ë¡œê·¸ ì¶œë ¥/í”Œë¡¯ ë¼ë²¨ë§ìš©)
    patience: int = 10,            # Early Stopping patience (val_loss ê°œì„  ì—†ì„ ì‹œ ì¤‘ë‹¨)

    # -------------------------
    # ì‹¤í–‰ í™˜ê²½
    # -------------------------
    seed: int = 42,                # ëœë¤ ì‹œë“œ (ì¬í˜„ì„± ë³´ì¥)
    device: str | None = None,     # ì—°ì‚° ì¥ì¹˜ ì§€ì • ("cuda", "cpu", Noneì´ë©´ ìë™)

    # -------------------------
    # noise setting
    # -------------------------
    noise: Optional[NoiseConfig] = None,        # ë…¸ì´ì¦ˆ êµ¬ì„± ê°ì²´ (label/feature ì¢…ë¥˜, ë¹„ìœ¨, ì‹œë“œ ë“±)
    noise_targets: Iterable[str] = ("train",),  # ë…¸ì´ì¦ˆ ì ìš© ëŒ€ìƒ split ("train","val","test" ì¤‘ ì„ íƒ)

    # -------------------------
    # outlier setting
    # -------------------------
    outliers: Optional[OutlierConfig] = None,   # outlier êµ¬ì„± ê°ì²´
):

return model, hist, dict(test_acc=test_acc, test_f1=test_f1, noise_meta=noise_meta, outlier_meta=outlier_meta)

```

#### **noise_meta**
```python
noise_meta: Dict[str, Any] = {
    "train": {   # train splitì— ë…¸ì´ì¦ˆ ì ìš© ì‹œ
        "kind": "label" | "feature" | "both",   # ì ìš©ëœ ë…¸ì´ì¦ˆ ì¢…ë¥˜
        # --- Label Noise ---
        "label_mode": "symmetric" | "pairflip" | "classdep" | "instancedep",
        "label_rate": float,        # ë…¸ì´ì¦ˆìœ¨ Î·
        "seed_label": int,          # ë¼ë²¨ ë…¸ì´ì¦ˆ ì‹œë“œ
        "label_idx": np.ndarray,    # ë³€ê²½ëœ ìƒ˜í”Œ ì¸ë±ìŠ¤
        "label_orig": np.ndarray,   # ë³€ê²½ ì „ ë¼ë²¨
        "transition": np.ndarray,   # ì „ì´í–‰ë ¬ (pairflip/classdep ëª¨ë“œì—ì„œ ì‚¬ìš©)

        # --- Feature Noise ---
        "feature_mode": "gaussian" | "spike",
        "seed_feature": int,        # í”¼ì²˜ ë…¸ì´ì¦ˆ ì‹œë“œ
        "feature_idx": np.ndarray,  # ë…¸ì´ì¦ˆê°€ ì ìš©ëœ ìƒ˜í”Œ ì¸ë±ìŠ¤
    },
    "val": None | {...},   # val splitì—ë„ ì ìš©ëœ ê²½ìš°
    "test": None | {...},  # test splitì—ë„ ì ìš©ëœ ê²½ìš°
}
```

#### **outlier_meta**
```python
outlier_meta: Dict[str, Any] = {
    "train": {   # train splitì— outlier ì ìš© ì‹œ
        "n_added": int,         # ì¶”ê°€ëœ outlier í–‰ ê°œìˆ˜
        "rate": float,          # ì „ì²´ ìƒ˜í”Œ ëŒ€ë¹„ outlier ë¹„ìœ¨
        "m_avg": float,         # í•œ í–‰ì—ì„œ ë³€ì¡°ëœ feature ìˆ˜ í‰ê· 
        "m_min": int,           # ë³€ì¡°ëœ feature ìˆ˜ ìµœì†Œ
        "m_max": int,           # ë³€ì¡°ëœ feature ìˆ˜ ìµœëŒ€

        # --- Z-score ìš”ì•½ (ì ˆëŒ€ê°’ ê¸°ì¤€) ---
        "z_avg": float,         # ëª¨ë“  outlier feature |z| í‰ê· 
        "z_std": float,         # ëª¨ë“  outlier feature |z| í‘œì¤€í¸ì°¨
        "z_min": float,         # ê´€ì¸¡ëœ |z| ìµœì†Œê°’
        "z_max": float,         # ê´€ì¸¡ëœ |z| ìµœëŒ€ê°’

        # --- Config ê¸°ë¡ ---
        "zmin": float,          # ì„¤ì •ëœ z-score í•˜í•œ
        "zmax": float,          # ì„¤ì •ëœ z-score ìƒí•œ
        "two_side": bool,       # Â± ë°©í–¥ í—ˆìš© ì—¬ë¶€
        "seed_outlier": int,    # outlier ìƒì„± ì‹œë“œ
    },
    "val": None | {...},   # val splitì—ë„ ì ìš©ëœ ê²½ìš°
    "test": None | {...},  # test splitì—ë„ ì ìš©ëœ ê²½ìš°
}
```

#### **run_clean_vs_noise**
```python
run_clean_vs_noise(
    df,                            # Dataframe
    schema_or_name,                # DatasetSchema ê°ì²´ ë˜ëŠ” registry.py ì˜ str
    *,
    loss_fn,                       # ì‚¬ìš©í•  ì†ì‹¤ í•¨ìˆ˜ (ì˜ˆ: CE, GCE, CCE ë“±)
    loss_name: str = "loss",       # ì†ì‹¤ í•¨ìˆ˜ ì´ë¦„ (ë¡œê·¸ ì¶œë ¥/í”Œë¡¯ ë¼ë²¨ë§ìš©)
    seed: int = 42,                # ëœë¤ ì‹œë“œ (ì¬í˜„ì„± ë³´ì¥)

    # -------------------------
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê¸°ë³¸ í”„ë¦¬ì…‹)
    # -------------------------
    epochs: int = 50,              # ìµœëŒ€ í•™ìŠµ epoch ìˆ˜
    batch_size: int = 64,          # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
    lr: float = 1e-3,              # í•™ìŠµë¥  (learning rate)
    weight_decay: float = 1e-4,    # L2 ì •ê·œí™” ê°•ë„ (weight decay)

    optimizer_name: str = "adam",  # ì˜µí‹°ë§ˆì´ì € ì¢…ë¥˜ ("adam" | "sgd" | "sgd_momentum")
    patience: int = 10,            # Early Stopping patience (val_loss ê°œì„  ì—†ì„ ì‹œ ì¤‘ë‹¨)
    device: str | None = None,

    # -------------------------
    # noise setting
    # -------------------------
    noise_cfg: Optional["NoiseConfig"] = None,
    noise_targets: Iterable[str] = ("train",),
):

return ( [hist_c, hist_n], ["CLEAN", "NOISE"], df_results )

```

#### **run_clean_vs_outlier**
```python
run_clean_vs_outlier(
    df,                               # ì „ì²´ ë°ì´í„°ì…‹ (pandas DataFrame)
    schema_or_name,                   # DatasetSchema ê°ì²´ ë˜ëŠ” str (ë“±ë¡ëœ ìŠ¤í‚¤ë§ˆ ì´ë¦„)

    *,
    loss_fn,                          # ì‚¬ìš©í•  ì†ì‹¤ í•¨ìˆ˜ (callable, ì˜ˆ: ce_loss, gce_loss ë“±)
    loss_name: str = "loss",          # ì†ì‹¤ í•¨ìˆ˜ ì´ë¦„ (ë¡œê·¸/ë¼ë²¨ë§ìš© í‘œì‹œ)

    seed: int = 42,                   # ëœë¤ ì‹œë“œ (ë°ì´í„° ë¶„í• /í•™ìŠµ ì¬í˜„ì„± ë³´ì¥)

    # -------------------------
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    # -------------------------
    epochs: int = 50,                 # ìµœëŒ€ í•™ìŠµ epoch ìˆ˜
    batch_size: int = 64,             # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
    lr: float = 1e-3,                 # í•™ìŠµë¥  (learning rate)
    weight_decay: float = 1e-4,       # L2 ì •ê·œí™” ê°•ë„ (weight decay)

    optimizer_name: str = "adam",     # ì˜µí‹°ë§ˆì´ì € ì¢…ë¥˜ ("adam" | "sgd" | "sgd_momentum")
    patience: int = 10,               # Early Stopping patience (val_loss ê°œì„  ì—†ì„ ì‹œ ì¤‘ë‹¨)

    # -------------------------
    # ì‹¤í–‰ í™˜ê²½
    # -------------------------
    device: str | None = None,        # ì—°ì‚° ì¥ì¹˜ ("cuda", "cpu", None â†’ ìë™ ê°ì§€)

    # -------------------------
    # ì•„ì›ƒë¼ì´ì–´ ì„¤ì •
    # -------------------------
    outlier_cfg: Optional[OutlierConfig] = None,  # OutlierConfig ê°ì²´ (rate, z ë²”ìœ„, m ë²”ìœ„ ë“± ì„¤ì •)
):

return ([hist_c, hist_o], ["CLEAN", "OUTLIER"], df_results)

```

---

## ğŸ“ Patch Notes
- 1.0.0: First release  
- 1.0.1: ë¡œê·¸ ìˆ˜ì •  
- 2.0.0: ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª¨ë“ˆí™”, ë…¸ì´ì¦ˆ ì¶”ê°€ ([íŒ¨ì¹˜ë‚´ì—­](https://github.com/RosePasta22/ML-DL-Seminar/releases/tag/v2.0.0))  
- 2.0.1: ë²„ê·¸ ìˆ˜ì •  
- 2.0.2: ë²„ê·¸ ìˆ˜ì •  
- 2.0.3: ë²„ê·¸ ìˆ˜ì •  
- 2.0.4: ë²„ê·¸ ìˆ˜ì • ([íŒ¨ì¹˜ë‚´ì—­](https://github.com/RosePasta22/ML-DL-Seminar/releases/tag/v2.0.4))  
- 2.1.0: Outliers ëª¨ë“ˆ ì¶”ê°€ ([íŒ¨ì¹˜ë‚´ì—­](https://github.com/RosePasta22/ML-DL-Seminar/releases/tag/v2.1.0))  
- 2.1.1: ì–•ì€ ë³µì‚¬ë¡œ ì¸í•œ ê²½ê³  í•´ê²° ([íŒ¨ì¹˜ë‚´ì—­](https://github.com/RosePasta22/ML-DL-Seminar/releases/tag/v2.1.1))  
- 2.1.2: Outlier ë©”íƒ€ ì¶”ê°€ ([íŒ¨ì¹˜ë‚´ì—­](https://github.com/RosePasta22/ML-DL-Seminar/releases/tag/v2.1.2))  
- 2.1.3: init ìˆ˜ì • ([íŒ¨ì¹˜ë‚´ì—­](https://github.com/RosePasta22/ML-DL-Seminar/releases/tag/v2.1.3))  
- 2.1.4: SCCE í•¨ìˆ˜ clamp 1ë¡œ ì¬ì •ì˜, êµ¬ë¶„ì„ ìœ„í•´ q_të¡œ ë³€ê²½ ([íŒ¨ì¹˜ë‚´ì—­](https://github.com/RosePasta22/ML-DL-Seminar/releases/tag/v2.1.4))  
- 2.1.5: PyPI íŒ¨í‚¤ì§• ì •ë¦¬, README ê°œì„ , ë°°í¬ëª… `robustloss-lab`, import `robustloss` ([íŒ¨ì¹˜ë‚´ì—­](https://github.com/RosePasta22/ML-DL-Seminar/releases/tag/v2.1.5))
- 2.1.6: Move package into new repo; update pyproject ([íŒ¨ì¹˜ë‚´ì—­](https://github.com/RosePasta22/Robustloss-Lab/releases/tag/v2.1.6))
- **2.1.7: Update README** ([íŒ¨ì¹˜ë‚´ì—­](https://github.com/RosePasta22/Robustloss-Lab/releases/tag/v2.1.7))

---

## ğŸ“ Links
- [Latest Release](https://github.com/RosePasta22/Robustloss-Lab/releases/tag/v2.1.7)  
- [Homepage / Source](https://github.com/RosePasta22/Robustloss-Lab)
